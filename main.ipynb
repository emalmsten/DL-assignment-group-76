{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 8103312,
     "sourceType": "datasetVersion",
     "datasetId": 4705127
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "from settings import sets\n",
    "from truncation.static_truncation import truncate_static\n",
    "from truncation.behaviour_truncation import truncate_behaviour\n",
    "\n",
    "# Change settings in settings file!\n",
    "# Restart kernel after changing settings!\n",
    "\n",
    "models = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"longformer\": \"allenai/longformer-base-4096\"\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    \"model_name\": models[sets[\"model_short_name\"]],\n",
    "    \"max_length\": sets[\"truncation_size\"],\n",
    "    \"batch_size\": 128,  # Set to 32 for Longformer due to GPU load\n",
    "    \"lr\": 5e-4,\n",
    "    \"epochs\": 7,\n",
    "    \"checkpoint_frequency\": 200,\n",
    "    \"cross_validation_folds\": 5,\n",
    "}\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "\n",
    "def process_data(data, tokenizer):\n",
    "    data_texts, data_labels = zip(*data)\n",
    "    data_encodings = tokenizer(list(data_texts), truncation=True, padding=True, max_length=params[\"max_length\"])\n",
    "    # also take sha\n",
    "    return MalwareDataset(data_encodings, labels=list(data_labels))\n",
    "\n",
    "\n",
    "# Gets the data from the fromatted json file and returns a training and test set\n",
    "def get_data(data, categories):\n",
    "    inverted_categories = {v: k for k, v in categories.items()}\n",
    "\n",
    "    # for all data make a list of tuples\n",
    "    data_list = []\n",
    "    name_list = []\n",
    "    for details in data:\n",
    "        index = inverted_categories[details.get('family_name')]\n",
    "        features = details.get('features_json')\n",
    "        data_list.append((features, index))\n",
    "\n",
    "        name_list.append(details.get('name'))\n",
    "\n",
    "    return data_list, name_list\n",
    "\n",
    "\n",
    "def format_data(data, features):\n",
    "    simplified_data = []\n",
    "    distinct_families = set()\n",
    "\n",
    "    if features == 'static':\n",
    "        for name, details in data.items():\n",
    "            distinct_families.add(details.get('family_name'))\n",
    "            simplified_data.append({\n",
    "                \"name\": name,\n",
    "                \"features_json\": details.get('features_json'),\n",
    "                \"family_name\": details.get('family_name')\n",
    "            })\n",
    "\n",
    "    elif features == 'behaviour':\n",
    "        for details in data:\n",
    "            distinct_families.add(details.get('Family Name'))\n",
    "            simplified_data.append({\n",
    "                \"name\": str(details.get('SHA')),\n",
    "                \"features_json\": str(details.get('Behavior')),\n",
    "                \"family_name\": str(details.get('Family Name'))\n",
    "            })\n",
    "\n",
    "    categories = {}\n",
    "    for i, family in enumerate(distinct_families):\n",
    "        categories[i] = family\n",
    "\n",
    "    return simplified_data, categories\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# gets the accuracy of the model\n",
    "def get_accuracy(preds, labels):\n",
    "    preds_tensor = torch.tensor(preds)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    correct = (preds_tensor == labels_tensor).float().sum()\n",
    "    accuracy = correct / len(labels)\n",
    "    return accuracy.item()  # Converts tensor to Python float\n",
    "\n",
    "\n",
    "def run_test(model, test_set, device):\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    print(\"Testing...\")\n",
    "\n",
    "    # DataLoader for test set\n",
    "    test_loader = DataLoader(test_set, batch_size=params['batch_size'], shuffle=False, )\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss.sum()\n",
    "\n",
    "            # Convert logits to probabilities to get the predicted class (highest probability)\n",
    "            all_preds.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(get_accuracy(all_preds, all_labels))\n",
    "\n",
    "    accuracy = get_accuracy(all_preds, all_labels)\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    print(f'Average loss: {avg_loss}')\n",
    "    print(f'Test Accuracy: {accuracy}')\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def train(model, optimizer, training_set, device, checkpoint_dir):\n",
    "    print(\"Training...\")\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    train_loader = DataLoader(training_set, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    print(\"Data loaded\")\n",
    "\n",
    "    for epoch in range(params[\"epochs\"]):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{params[\"epochs\"]}'):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss = loss.sum()\n",
    "            loss.backward()\n",
    "\n",
    "            all_preds.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(get_accuracy(all_preds, all_labels))\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\n",
    "            f'Epoch {epoch + 1}/{params[\"epochs\"]}, average loss: {sum(losses) / len(losses)}, average accuracy: {sum(accuracies) / len(accuracies)}')\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    save_checkpoint(model, optimizer, checkpoint_dir, filename=\"final_checkpoint.pth.tar\")\n",
    "\n",
    "\n",
    "# saves the model and optimizer to a checkpoint, can later be used\n",
    "def save_checkpoint(model, optimizer, checkpoint_dir, filename=\"checkpoint.pth.tar\"):\n",
    "    # Ensure directory exists\n",
    "    filename = os.path.join(checkpoint_dir, filename)\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, filename)\n",
    "\n",
    "\n",
    "# loads a checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, device):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Load the saved model and optimizer states\n",
    "    model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Move optimizer state to device\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "\n",
    "    return model, optimizer"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from math import sqrt\n",
    "\n",
    "params = hyperparams\n",
    "\n",
    "# external imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.device_count()\n",
    "\n",
    "checkpoint_dir = f'checkpoints/{sets[\"model_short_name\"]}_{time.strftime(\"%Y-%m-%d-%H-%M-%S\")}'\n",
    "\n",
    "feature_type = \"behaviour\" if sets[\"file_name\"] == \"behavior_features\" else \"static\"\n",
    "\n",
    "input_file = f'data/truncated_{sets[\"file_name\"]}_to_{sets[\"truncation_size\"]}.json'\n",
    "\n",
    "\n",
    "def summarize_results(accuracies, losses, n_folds, best_acc, fold, output_file):\n",
    "    avg_acc = round(np.average(accuracies), 3)\n",
    "    sd_acc = round(np.std(accuracies), 3)\n",
    "    sem_acc = round(sd_acc / sqrt(n_folds), 3)\n",
    "\n",
    "    avg_loss = round(np.average(losses), 3)\n",
    "    sd_loss = round(np.std(losses), 3)\n",
    "    sem_loss = round(sd_loss / sqrt(n_folds), 3)\n",
    "    best_acc = round(best_acc, 3)\n",
    "\n",
    "    accuracies = [round(acc, 3) for acc in accuracies]\n",
    "    losses = [round(loss, 3) for loss in losses]\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(f\"Best fold: {fold}\\n\\n\")\n",
    "        file.write(f\"Accuracies: {accuracies}\\n\")\n",
    "        file.write(f\"Average: {avg_acc}\\n\")\n",
    "        file.write(f\"Standard deviation: {sd_acc}\\n\")\n",
    "        file.write(f\"Standard error of the mean: {sem_acc}\\n\")\n",
    "        file.write(f\"Best accuracy: {best_acc}\\n\\n\")\n",
    "\n",
    "        file.write(f\"Losses: {losses}\\n\")\n",
    "        file.write(f\"Average: {avg_loss}\\n\")\n",
    "        file.write(f\"Standard deviation: {sd_loss}\\n\")\n",
    "        file.write(f\"Standard error of the mean: {sem_loss}\\n\")\n",
    "\n",
    "def get_embeddings(model, data_set, name_list):\n",
    "    model.eval()\n",
    "    embeddings = {}\n",
    "\n",
    "    for idx, sha in enumerate(tqdm(name_list)):\n",
    "        data = data_set[idx]\n",
    "        embedding_json = {}\n",
    "        with torch.no_grad():\n",
    "            input_ids = data['input_ids'].unsqueeze(0).to(device)\n",
    "            attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n",
    "\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            hidden_states = output.hidden_states[-1].mean(dim=1).squeeze().tolist()\n",
    "            embedding_json[\"embeddings\"] = hidden_states\n",
    "            embedding_json[\"family\"] = data[\"labels\"].item()\n",
    "            embeddings[sha] = embedding_json\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def run_cross_validation():\n",
    "    print(\"==== CROSS VALIDATION ====\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.device_count()\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    print(sets)\n",
    "    print(params)\n",
    "\n",
    "    if sets[\"truncation_needed\"]:\n",
    "        print(\"Truncating...\")\n",
    "        truncate_behaviour() if feature_type == \"behaviour\" else truncate_static()\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data, categories = format_data(data, feature_type)\n",
    "\n",
    "    random.Random(69).shuffle(data)\n",
    "\n",
    "    num_labels = len(categories)\n",
    "    n_folds = params[\"cross_validation_folds\"]\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    model_name = params['model_name']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    data, name_list = get_data(data, categories)\n",
    "\n",
    "    test_size = int(len(data) / n_folds)\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    best_acc = -1\n",
    "    best_model = None\n",
    "    best_optimizer = None\n",
    "    best_fold = -1\n",
    "\n",
    "    test_only = sets[\"test_only\"]\n",
    "    data[:] = process_data(data[:], tokenizer)\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        print(f\"--- Fold {i + 1}/{n_folds} ---\")\n",
    "\n",
    "        # Split the data and process it\n",
    "        start = test_size * i\n",
    "        end = start + test_size\n",
    "\n",
    "        training_set = data[:start] + data[end:]\n",
    "        test_set = data[start:end]\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        model.to(device)\n",
    "        model = nn.DataParallel(model)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "        # Freeze all layers except the classifier\n",
    "        for param in model.module.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        local_model = sets[\"local_model\"]\n",
    "        # If a local model should be run instead of the model from the Hugging Face model hub\n",
    "        if local_model is not None:\n",
    "            print(f\"Loading local model {local_model}\")\n",
    "            model, optimizer = load_checkpoint(local_model, model, optimizer, device)\n",
    "\n",
    "        # make sure it runs on cuda\n",
    "        model.to(device)\n",
    "\n",
    "        print(\"Model and tokenizer initialized\")\n",
    "\n",
    "        if not test_only:\n",
    "            checkpoint_dir_fold = checkpoint_dir + f\"/fold_{i + 1}\"\n",
    "            Path(checkpoint_dir_fold).mkdir(parents=True, exist_ok=True)\n",
    "            train(model, optimizer, training_set, device, checkpoint_dir_fold)\n",
    "        acc, loss = run_test(model, test_set, device)\n",
    "        accuracies.append(acc)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model = model\n",
    "            best_optimizer = optimizer\n",
    "            best_test = test_set\n",
    "            print(f\"New best acc {best_acc}\")\n",
    "            best_fold = i + 1\n",
    "\n",
    "    summarize_results(accuracies, losses, n_folds, best_acc, best_fold, checkpoint_dir + \"/results.txt\")\n",
    "    checkpoint_dir_best = checkpoint_dir + \"/best\"\n",
    "    Path(checkpoint_dir_best).mkdir(parents=True, exist_ok=True)\n",
    "    save_checkpoint(best_model, best_optimizer, checkpoint_dir_best, filename=\"best_model.pth.tar\")\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    embeddings = get_embeddings(best_model.module.base_model, data, name_list)\n",
    "    output_file = checkpoint_dir + \"/embeddings.json\"\n",
    "\n",
    "    print(\"Writing embeddings...\")\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(embeddings, file, indent=4)\n",
    "\n",
    "    return best_model, best_test\n",
    "\n",
    "\n",
    "best_model, best_test = run_cross_validation()\n",
    "print(\"All done!\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
